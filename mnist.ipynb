{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from hybrid_network import *\n",
    "from hybrid_util import *\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = get_mnist()                                \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "net = MNIST_Large_ConvNet(\n",
    "    non_negative = [False, False, False, False, False, False, False], \n",
    "    norm = [False, False, False, False, False, False, False])\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15000,25000], gamma=0.1)\n",
    "eps = 2/255 * 1.1\n",
    "running_eps = 0\n",
    "epoch = 0\n",
    "itr = 0\n",
    "k = 0\n",
    "while itr < 60000:\n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        net.train() \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(torch.cat([inputs, inputs], 0))\n",
    "        outputs  = outputs[:outputs.shape[0]//2]\n",
    "        loss += (1 - k) * criterion(outputs, labels)\n",
    "        \n",
    "        if itr > 2000 and itr < 12000:\n",
    "            running_eps += eps/10000\n",
    "            k += 0.5/10000\n",
    "   \n",
    "        if itr > 2000:\n",
    "            x_ub = inputs + running_eps\n",
    "            x_lb = inputs - running_eps\n",
    "            \n",
    "            outputs = net.forward(torch.cat([x_ub, x_lb], 0))\n",
    "            z_hb = outputs[:outputs.shape[0]//2]\n",
    "            z_lb = outputs[outputs.shape[0]//2:]\n",
    "            lb_mask = torch.eye(10).cuda()[labels]\n",
    "            hb_mask = 1 - lb_mask\n",
    "            outputs = z_lb * lb_mask + z_hb * hb_mask\n",
    "            loss += k * criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        itr+=1\n",
    "        running_loss += loss.item()\n",
    "    print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 600))\n",
    "    net.eval()\n",
    "    print_accuracy(net, trainloader, testloader, device, test=True, eps = 0)\n",
    "    print_accuracy(net, trainloader, testloader, device, test=True, eps = running_eps)\n",
    "    if itr > 25000:\n",
    "        print_accuracy(net, trainloader, testloader, device, test=True, eps = 2/255)\n",
    "    epoch+= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
